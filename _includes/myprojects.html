
          <div class="myprojects">
            <h2>Projects</h2>

            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

              <!-- VISION BASED NAVIGATION FOR MID-AIR HELICOPTER DELIVERY -->
              <tr>
                <td width="40%" valign="top" align="center">
                <img src="assets/img/nasajpl_xVIOtracking.gif" alt="sym" width="90%" style="margin-top:10px;padding-top:0px;padding-bottom:0px;border-radius:15px;border:1px solid black">
                </td>

                <td width="60%" valign="top">
                  <p><a href="nasajpl.html" id="plr">
                  <papertitle><b>Vision-Based Navigation for Mid-Air Helicopter Delivery on Mars</b></papertitle></a><br>
                    Supervised by <a href="https://www-robotics.jpl.nasa.gov/who-we-are/people/jeff_delaune/">Jeff Delaune</a>,
                    <a href="https://giovanni-cioffi.netlify.app/">Giovanni Cioffi</a>
                     and <a href="https://rpg.ifi.uzh.ch/people_scaramuzza.html">Davide Scaramuzza</a>
                  </p>

                  <p>              
                  <div class="project" id="plr">
                    <a href="plr.html">Webpage</a> |
                    <a href="assets/pdf/plr_poster.pdf">Poster</a> |
                    <a href="https://www.youtube.com/watch?v=4saCcrD37qA">Video</a> |
                    <a href="assets/pdf/plr_report.pdf">Report</a> 
                    <!-- <a href="https://github.com/leggedrobotics/anymal_ar">Code (Open Soon)</a> -->
                  </div>
                  </p>

                  <p>
                    <div>
                      A novel range-VIO method that fuses altimeter measurements with a visual-odometry framework, and eliminates 
                      the need for any type of ground planarity assumption, making it adaptable to any terrain structure, while 
                      still being able to observe scale and mitigate error drift under constant-velocity motion and without 
                      relying on prior maps.
                    </div>
                  </p>
                            
              </td>
              </tr>


              <!-- PERCEPTION AND LEARNING FOR ROBOTICS PROJECT -->
              <tr>
                <td width="40%" valign="top" align="center">
                <img src="assets/img/plr.gif" alt="sym" width="90%" style="margin-top:10px;padding-top:0px;padding-bottom:0px;border-radius:15px;border:1px solid black">
                </td>

                <td width="60%" valign="top">
                  <p><a href="plr.html" id="plr">
                  <papertitle><b>Monocular Markerless 6D Pose Estimation of ANYmal</b></papertitle></a><br>
                    Team members:
                    <b>Ivan Alberico</b>,
                    <a href="https://tenhearts.github.io/">Kexin Shi</a>
                    <br>
                    Supervised by <a href="https://mavt.ethz.ch/people/person-detail.MjY0MDU2.TGlzdC81NTksLTE3MDY5NzgwMTc=.html">Jonas Frey</a>,
                     <a href="https://mavt.ethz.ch/people/person-detail.MjE4MDk1.TGlzdC81NTksLTE3MDY5NzgwMTc=.html">Victor Klemm</a>
                     and <a href="https://ethz.ch/en/the-eth-zurich/organisation/who-is-who/mavt/details.MTIxOTEx.TGlzdC8xOTEyLDEwNjE0ODE1NjU=.html">Marco Hutter</a>
                  </p>

                  <p>              
                  <div class="project" id="plr">
                    <a href="plr.html">Webpage</a> |
                    <a href="assets/pdf/plr_poster.pdf">Poster</a> |
                    <a href="https://www.youtube.com/watch?v=4saCcrD37qA">Video</a> |
                    <a href="assets/pdf/plr_report.pdf">Report</a> 
                    <!-- <a href="https://github.com/leggedrobotics/anymal_ar">Code (Open Soon)</a> -->
                  </div>
                  </p>

                  <p>
                    <div>
                      An accurate toolbox for localization and pose estimation of <a href="https://www.anybotics.com/anymal-autonomous-legged-robot/">ANYmal</a> without 
                      external sources like depth cameras or QR codes.
                    </div>
                  </p>
                            
              </td>
              </tr>


              <!-- LEARNING TO GENERATE EVENTS USING SPIKING NEURAL NETWORKS -->
              <tr>
                <td width="40%" valign="top" align="center">
                <img src="assets/img/semesterproject.gif" alt="sym" width="90%" style="margin-top:10px;padding-top:0px;padding-bottom:0px;border-radius:15px;border:1px solid black">
                </td>

                <td width="60%" valign="top">
                  <p><a href="semesterproject.html" id="plr">
                  <papertitle><b>Learning to Generate Events using Spiking Neural Networks</b></papertitle></a><br>
                    Supervised by <a href="https://danielgehrig18.github.io/">Daniel Gehrig</a>,
                     and <a href="https://rpg.ifi.uzh.ch/people_scaramuzza.html">Davide Scaramuzza</a>
                  </p>

                  <p>              
                  <div class="project" id="plr">
                    <a href="plr.html">Webpage</a> |
                    <a href="assets/pdf/plr_poster.pdf">Poster</a> |
                    <a href="https://www.youtube.com/watch?v=4saCcrD37qA">Video</a> |
                    <a href="assets/pdf/plr_report.pdf">Report</a> 
                    <!-- <a href="https://github.com/leggedrobotics/anymal_ar">Code (Open Soon)</a> -->
                  </div>
                  </p>

                  <p>
                    <div>
                      A learning-based solution that converts any existing video dataset recorded with 
                      conventional cameras to synthetic event data. 
                    </div>
                  </p>
                            
              </td>
              </tr>
              <!-- DEEP LEARNING FOR AUTONOMOUS DRIVING PROJECT -->
              <tr>
                <td width="40%" valign="top" align="center">
                <img src="assets/img/dlad.jpg" alt="sym" width="90%" style="margin-top:10px;padding-top:0px;padding-bottom:0px;border-radius:15px;border:1px solid black">
                </td>

                <td width="60%" valign="top">
                  <p><a href="dlad.html" id="dlad">
                  <papertitle><b>Multi-task Learning for Autonomous Driving</b></papertitle></a><br>
                  </p>

                  <p>              
                  <div class="project" id="dlad">
                    <a href="dlad.html">Webpage</a>
                  </div>
                  </p>
                    
                  <p>
                    <div>
                      Multi-task Learning for semantic segmentation, depth estimation and 3D object detection for autonomous driving scenes. 
                    </div>
                  </p>
                            
              </td>
              </tr>
              
            </tbody></table>

          </div>
