
          <div class="myprojects">
            <h2>Projects</h2>

            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

              <!-- VISION BASED NAVIGATION FOR MID-AIR HELICOPTER DELIVERY -->
              <tr>
                <td width="40%" valign="top" align="center">
                <img src="assets/img/nasajpl_xVIOtracking.gif" alt="sym" width="90%" style="margin-top:10px;padding-top:0px;padding-bottom:0px;border-radius:15px;border:1px solid black">
                </td>

                <td width="60%" valign="top">
                  <p><a href="nasajpl.html" id="plr">
                  <papertitle><b>Vision-Based Navigation for Mid-Air Helicopter Delivery on Mars</b></papertitle></a><br>
                    Supervised by <a href="https://www-robotics.jpl.nasa.gov/who-we-are/people/jeff_delaune/">Jeff Delaune</a>,
                    <a href="https://giovanni-cioffi.netlify.app/">Giovanni Cioffi</a>
                     and <a href="https://rpg.ifi.uzh.ch/people_scaramuzza.html">Davide Scaramuzza</a>
                  </p>

                  <p>              
                  <div class="project" id="plr">
                    <a href="plr.html">Webpage</a> |
                    <a href="assets/pdf/plr_poster.pdf">Poster</a> |
                    <a href="https://www.youtube.com/watch?v=4saCcrD37qA">Video</a> |
                    <a href="assets/pdf/plr_report.pdf">Report</a> 
                    <!-- <a href="https://github.com/leggedrobotics/anymal_ar">Code (Open Soon)</a> -->
                  </div>
                  </p>

                  <p>
                    <div>
                      A novel range-VIO method that fuses altimeter measurements with a visual-odometry framework, and eliminates 
                      the need for any type of ground planarity assumption, making it adaptable to any terrain structure, while 
                      still being able to observe scale and mitigate error drift under constant-velocity motion and without 
                      relying on prior maps.
                    </div>
                  </p>
                            
              </td>
              </tr>



              <!-- INSTINCTIVE ROBOT CONTROL VIA HOLOLENS 2 -->
              <tr>
                <td width="40%" valign="top" align="center">
                <img src="assets/img/mixedreality_wrapup.gif" alt="sym" width="90%" style="margin-top:10px;padding-top:0px;padding-bottom:0px;border-radius:15px;border:1px solid black">
                </td>

                <td width="60%" valign="top">
                  <p><a href="mixedreality.html" id="plr">
                  <papertitle><b>Instinctive Robot Control via Hololens2</b></papertitle></a><br>
                  Team members:
                  <b>Ivan Alberico</b>,
                  <a href="https://www.linkedin.com/in/michael--baumgartner/?originalSubdomain=ch">Michael Baumgartner</a>,
                  <a href="https://www.linkedin.com/in/jonathan-becker-tech/">Jonathan Becker</a>,
                  <a href="https://www.linkedin.com/in/seif-ismail-143b53247/">Seif Ismail</a>
                  <br>
                  </p>

                  <p>              
                  <div class="project" id="plr">
                    <a href="mixedreality.html">Webpage</a> |
                    <a href="https://www.youtube.com/watch?v=YiZyG_5g66w">Video</a> |
                    <a href="assets/pdf/mixedreality_report.pdf">Report</a> 
                    <a href="https://github.com/MR-Instinctive-Robot-Control/Hand-Robot-Controller">Code </a>
                  </div>
                  </p>

                  <p>
                    <div>
                      The goal of the project is to develop an intuitive mixed reality interface on a Microsoft 
                      Hololens 2, with which the user is able to remotely control a robotic arm and perform basic 
                      assembly tasks using hand and eye tracking. The project requires the use of C#/Unity/MRTK 
                      for interfacing with the HL2, ROS for tele-communicating with the physical robot and Python/OpenCV 
                      for estimating objects' poses (using ArUco markers) in the physical environment and mapping 
                      them to the MR environment of the user.
                    </div>
                  </p>
                            
              </td>
              </tr>



              <!-- PERCEPTION AND LEARNING FOR ROBOTICS PROJECT -->
              <tr>
                <td width="40%" valign="top" align="center">
                <img src="assets/img/plr.gif" alt="sym" width="90%" style="margin-top:10px;padding-top:0px;padding-bottom:0px;border-radius:15px;border:1px solid black">
                </td>

                <td width="60%" valign="top">
                  <p><a href="plr.html" id="plr">
                  <papertitle><b>Monocular Markerless 6D Pose Estimation of ANYmal</b></papertitle></a><br>
                    Team members:
                    <b>Ivan Alberico</b>,
                    <a href="https://tenhearts.github.io/">Kexin Shi</a>
                    <br>
                    Supervised by <a href="https://mavt.ethz.ch/people/person-detail.MjY0MDU2.TGlzdC81NTksLTE3MDY5NzgwMTc=.html">Jonas Frey</a>,
                     <a href="https://mavt.ethz.ch/people/person-detail.MjE4MDk1.TGlzdC81NTksLTE3MDY5NzgwMTc=.html">Victor Klemm</a>
                     and <a href="https://ethz.ch/en/the-eth-zurich/organisation/who-is-who/mavt/details.MTIxOTEx.TGlzdC8xOTEyLDEwNjE0ODE1NjU=.html">Marco Hutter</a>
                  </p>

                  <p>              
                  <div class="project" id="plr">
                    <a href="plr.html">Webpage</a> |
                    <a href="assets/pdf/plr_poster.pdf">Poster</a> |
                    <a href="https://www.youtube.com/watch?v=4saCcrD37qA">Video</a> |
                    <a href="assets/pdf/plr_report.pdf">Report</a> 
                    <!-- <a href="https://github.com/leggedrobotics/anymal_ar">Code (Open Soon)</a> -->
                  </div>
                  </p>

                  <p>
                    <div>
                      An accurate toolbox for localization and pose estimation of <a href="https://www.anybotics.com/anymal-autonomous-legged-robot/">ANYmal</a> without 
                      external sources like depth cameras or QR codes.
                    </div>
                  </p>
                            
              </td>
              </tr>


              <!-- LEARNING TO GENERATE EVENTS USING SPIKING NEURAL NETWORKS -->
              <tr>
                <td width="40%" valign="top" align="center">
                <img src="assets/img/semesterproject.gif" alt="sym" width="90%" style="margin-top:10px;padding-top:0px;padding-bottom:0px;border-radius:15px;border:1px solid black">
                </td>

                <td width="60%" valign="top">
                  <p><a href="semesterproject.html" id="plr">
                  <papertitle><b>Learning to Generate Events using Spiking Neural Networks</b></papertitle></a><br>
                    Supervised by <a href="https://danielgehrig18.github.io/">Daniel Gehrig</a>,
                     and <a href="https://rpg.ifi.uzh.ch/people_scaramuzza.html">Davide Scaramuzza</a>
                  </p>

                  <p>              
                  <div class="project" id="plr">
                    <a href="plr.html">Webpage</a> |
                    <a href="assets/pdf/plr_poster.pdf">Poster</a> |
                    <a href="https://www.youtube.com/watch?v=4saCcrD37qA">Video</a> |
                    <a href="assets/pdf/plr_report.pdf">Report</a> 
                    <!-- <a href="https://github.com/leggedrobotics/anymal_ar">Code (Open Soon)</a> -->
                  </div>
                  </p>

                  <p>
                    <div>
                      A learning-based solution that converts any existing video dataset recorded with 
                      conventional cameras to synthetic event data. 
                    </div>
                  </p>
                            
              </td>
              </tr>
              <!-- DEEP LEARNING FOR AUTONOMOUS DRIVING PROJECT -->
              <tr>
                <td width="40%" valign="top" align="center">
                <img src="assets/img/dlad.jpg" alt="sym" width="90%" style="margin-top:10px;padding-top:0px;padding-bottom:0px;border-radius:15px;border:1px solid black">
                </td>

                <td width="60%" valign="top">
                  <p><a href="dlad.html" id="dlad">
                  <papertitle><b>Multi-task Learning for Autonomous Driving</b></papertitle></a><br>
                  </p>

                  <p>              
                  <div class="project" id="dlad">
                    <a href="dlad.html">Webpage</a>
                  </div>
                  </p>
                    
                  <p>
                    <div>
                      Multi-task Learning for semantic segmentation, depth estimation and 3D object detection for autonomous driving scenes. 
                    </div>
                  </p>
                            
              </td>
              </tr>
              
            </tbody></table>

          </div>
