<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> | About me</title> <meta name="author" content="Ivan Alberico"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/autumn.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/icon.png"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://ivanalberico2.github.io//semesterproject.html"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/">About me</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About</a> </li> <li class="nav-item"> <a class="nav-link " href="/assets/pdf/Ivan_CV.pdf"><span>CV</span></a> </li> <li class="nav-item"> <a class="nav-link " href="/assets/pdf/Ivan_CV.pdf"><span>CV</span></a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <meta charset="UTF-8"> <meta name="viewport" content="width=device-width, initial-scale=1.0"> <style>.justified-text{text-align:justify}</style> <title>Justified Text Example</title> <section class="hero"> <div class="hero-body"> <div class="container is-max-desktop"> <div class="column has-text-centered"> <h1 align="center" class="title is-1 publication-title"><b>Learning to Generate Events <br>using Spiking Neural Networks</b></h1> <div class="column is-full_width"> </div> <br> <div align="center" class="is-size-5 publication-authors"> <h5 align="center" class="title is-1 publication-title"> <b> <a href="https://ivanalberico.github.io/" rel="external nofollow noopener" target="_blank">Ivan Alberico</a>     <span class="author-block"> <a href="https://danielgehrig18.github.io/" rel="external nofollow noopener" target="_blank">Daniel Gehrig</a></span>     <span class="author-block"> <a href="https://rpg.ifi.uzh.ch/people_scaramuzza.html" rel="external nofollow noopener" target="_blank">Davide Scaramuzza</a></span>     </b> <br><br> <span class="author-block"><a href="https://rpg.ifi.uzh.ch/index.html" rel="external nofollow noopener" target="_blank">Robotics and Perception Group (RPG)</a></span> </h5> </div> </div> </div> </div> </section> </div> <section class="section"> <div class="container is-max-desktop"> <br><hr> <div class="columns is-centered has-text-centered"> <div class="column is-four-fifths"> <br> <h2 align="center" class="title is-3"><b>Abstract</b></h2> <div class="content has-text-justified"> <p class="justified-text"> The recent advent of event cameras in computer vision applications has significantly increased the performance of traditional methods, by leveraging the outstanding advantages of these novel sensors over conventional cameras. If on one hand the event-based techniques have achieved impressive results, on the other the same methods are nonetheless limited by the scarce amount of event data required for training. In this work, we propose a learning-based solution relying on Spiking Neural Networks that addresses this issue by generating event data starting from the huge amount of pre-existing video datasets recorded with conventional cameras. The Spiking Neural Networks leverage the asynchronous nature of events to generate event data in an end-to-end fashion, starting from high temporal resolution videos. We evaluate the method on different levels: </p> <li>a visual inspection of the generated events compared to the ground truth ones;</li> <li>an in-depth analysis of the event rates of the generated events;</li> <li>an object classification task validated on the N-Caltech101 dataset.</li> <p class="justified-text"> </p> <br> </div> </div> </div> <section class="section"> <div class="container is-max-desktop"> <div class="columns is-centered has-text-centered"> <div class="column is-four-fifths"> <hr> <br> <h2 align="center" class="title is-3"><b>Key Idea</b></h2> <div class="content has-text-justified"> <br> <p class="justified-text"> Research in this area has shown that computer vision techniques based on event data have achieved outstanding results, leading to a growing interest in this field. However, one the main limitations of these methods is that they require a large amount of event data for training, which is not available for mainly two reasons: first, because these sensors have been recently introduced in the market and secondly because they are rather expensive, making them not accessible to everyone. </p> <p class="justified-text"> </p> <p class="justified-text"> The goal of this project is to design a learning-based solution that addresses this issue by converting any existing frame-based dataset recorded with conventional cameras into synthetic event data. In this way, we can leverage the enormous amount of video datasets to automatically generate events. The general pipeline of the project can be summarized in the following steps: starting from low resolution videos, <a href="https://rpg.ifi.uzh.ch/TimeLens.html/" rel="external nofollow noopener" target="_blank">TimeLens</a> frame interpolation technique is first deployed to generate high resolution videos which are directly fed into the event generation network. The training process of the Spiking Neural Network is supervised by a spike loss relying on ground truth events. To summarize, the main contributions are: </p> <li>We propose a learning-based solution to generate synthetic events starting from video sequences.</li> <li>We show that the events generated with this method accurately reproduce the corresponding real events in most of the cases, although being sensitive to noise.</li> <li>We evaluate our method on an object classification task and show that models trained on the synthetic events generated with the proposed network perform better than <a href="https://tub-rip.github.io/eventvision2021/papers/2021CVPRW_V2E_From_Video_Frames_to_Realistic_DVS_Events.pdf" rel="external nofollow noopener" target="_blank">Vid2E</a> in terms of accuracy.</li> <p class="justified-text"> </p> <div align="center"><img src="assets/img/semesterproject_generalidea.png" width="80%" class="center"></div> <p class="justified-text"> </p> </div> </div> </div> <hr> <div class="columns is-centered has-text-centered"> <div class="column is-four-fifths"> <br> <h2 align="center" class="title is-3"><b>Method</b></h2> <br> <div class="content has-text-justified"> <p class="justified-text"> In this section we present Spike-ESIM, a novel Spiking Neural Network for event generation from video frames. The proposed network is different from traditional Spiking Neural Networks in the way that it does not take as input trains of asynchronous spikes, but rather sequences of upsampled video frames which are fed directly at the level of the membrane potential of the first layer. In this way, we skip the Post Synaptic Potential layer at the initial stage of the network, which normally convolves the input spikes of each node with the temporal ε(·) kernel to generate the potentials. </p> <p class="justified-text"> </p> <div align="center"><img src="assets/img/semesterproject_dynamicsneuron.png" width="60%" class="center"></div> <p class="justified-text"> </p> <p class="justified-text"> The network is a branched architecture, with each branch modeling a specific polarity of the output event tensor. Modeling the event generation process with a single network would not be feasible, since we need to account for both positive and negative changes of brightness along the frames. The way the input video frames are fed into the network is the following: starting from a sequence of T consecutive frames, we compute both the positive and negative difference of the intensities along adjacent frames, namely ∆I(t) and −∆I(t), and we feed them into the two separate branches. Each branch is composed of a 3D convolution layer followed by a spike function module, this latter being the one that actually generates events by thresholding the scaled differences of brightness according to the membrane potential hyper-parameter. </p> <p class="justified-text"> </p> <div align="center"><img src="assets/img/semesterproject_spikeESIMarch.png" width="60%" class="center"></div> <p class="justified-text"> </p> </div> </div> </div> <hr> <div class="columns is-centered has-text-centered"> <div class="column is-four-fifths"> <br> <h2 align="center" class="title is-3"><b>Results</b></h2> <br> <div class="content has-text-justified"> <p class="justified-text"> In this chapter we provide experimental results to validate the effectiveness of the proposed method. A qualitative evaluation is performed by visually comparing the generated events to the ground truth events. Visualizing the generated events reveals a strong dependency of the method on background noise present in most of the scenes of the HS-ERGB dataset. The amount of noise that is modeled in the scene is strongly dependent on the membrane threshold that we set before training, and its value is extremely important as it strikes a balance between how much noise we allow in the results and how well we want to model the real events </p> <p class="justified-text"> </p> <div align="center"><img src="assets/img/semesterproject_result1.png" width="60%" class="center"></div> <p class="justified-text"> </p> <div align="center"><img src="assets/img/semesterproject_result2.png" width="60%" class="center"></div> <p class="justified-text"> </p> </div> <br> </div> </div> </div> </section> </div></section> </body> </html><html> <footer class="sticky-bottom mt-5"> <div class="container"> © Copyright 2024 Ivan Alberico. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme.Last updated: October 03, 2024. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </html>