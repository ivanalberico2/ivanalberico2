---
layout: default
---

<!-- <!DOCTYPE html> -->
<html>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <!-- <div class="columns is-centered">
        <div class="column is-7 has-text-centered">
          <img src="media/nice-slam/nice-slam-logo2.png" alt="NICE-SLAM"/>
        </div>
      </div> -->
        <div class="column has-text-centered">
          <!-- <h1 class="title is-1 publication-title"><img src="media/nice-slam/like.png" width="90">NICE-SLAM&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>Neural Implicit Scalable Encoding for SLAM</h1> -->
          <!-- <h1 class="title is-1 publication-title"><img src="media/openscene/logo.png" width="70">OpenScene&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</h1> -->
          <h1 align="center" class="title is-1 publication-title"><b>Learning to Generate Events <br>using Spiking Neural Networks</b></h1>
          <div class="column is-full_width">
            <!-- <h2 align="center" class="title is-4">Master Project</h2> -->
          </div>
          <br>
          <div align="center" class="is-size-5 publication-authors">
              <!-- <span class="author-block"> -->
                <h5 align="center" class="title is-1 publication-title">
                  <b>
                <!-- <a href="https://pengsongyou.github.io">Songyou Peng</a><sup>1,2,3 * </sup></span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; -->
                <a href="https://ivanalberico.github.io/">Ivan Alberico</a></span></span>&nbsp;&nbsp;&nbsp;&nbsp;
              <!-- </span> -->
              <span class="author-block">
              <a href="https://danielgehrig18.github.io/">Daniel Gehrig</a></span>&nbsp;&nbsp;&nbsp;&nbsp;
              </span>
            <span class="author-block">
              <a href="https://rpg.ifi.uzh.ch/people_scaramuzza.html">Davide Scaramuzza</a></span>&nbsp;&nbsp;&nbsp;&nbsp;
            </span>
            </b>
            <br><br>
            <span class="author-block"><a href="https://rpg.ifi.uzh.ch/index.html/">Robotics and Perception Group (RPG)</a></span>
            </h5>
          </div>
          </div>
        </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- <div align="center"><img src="assets/img/mp.gif" width="90%" class="center"/></div> -->
    <br><hr>
      <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <br>
        <h2 align="center" class="title is-3"><b>Abstract</b></h2>
        <div class="content has-text-justified">
          <p>
            The recent advent of event cameras in computer vision applications has significantly increased 
            the performance of traditional methods, by leveraging the outstanding advantages of these novel 
            sensors over conventional cameras. If on one hand the event-based techniques have achieved 
            impressive results, on the other the same methods are nonetheless limited by the scarce amount 
            of event data required for training. In this work, we propose a learning-based solution relying 
            on Spiking Neural Networks that addresses this issue by generating event data starting from the 
            huge amount of pre-existing video datasets recorded with conventional cameras. The Spiking Neural 
            Networks leverage the asynchronous nature of events to generate event data in an end-to-end fashion, 
            starting from high temporal resolution videos. We evaluate the method on different levels:
            
            <li>a visual inspection of the generated events compared to the ground truth ones;</li>
            <li>an in-depth analysis of the event rates of the generated events;</li>
            <li>an object classification task validated on the N-Caltech101 dataset.</li>

          </p>

          <br>
        </div>
      </div>
    </div>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Key Idea -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <hr>
        <br>
        <h2 align="center" class="title is-3"><b>Key Idea</b></h2>

        <div class="content has-text-justified">
          <br>  
          <p>
            Research in this area has shown that computer vision techniques based on 
            event data have achieved outstanding results, leading to a growing interest 
            in this field. However, one the main limitations of these methods is that 
            they require a large amount of event data for training, which is not available 
            for mainly two reasons: first, because these sensors have been recently 
            introduced in the market and secondly because they are rather expensive, 
            making them not accessible to everyone. The goal of this project is to design 
            a learning-based solution that addresses this issue by converting any existing 
            frame-based dataset recorded with conventional cameras into synthetic event data. 
            In this way, we can leverage the enormous amount of video datasets to automatically 
            generate events.
            
            The general pipeline of the project can be summarized in the following steps: 
            starting from low resolution videos, <a href="https://rpg.ifi.uzh.ch/TimeLens.html/">TimeLens</a> 
            frame interpolation technique is first deployed to generate high resolution videos
            which are directly fed into the event generation network. The training process of 
            the Spiking Neural Network is supervised by a spike loss relying on ground truth 
            events. To summarize, the main contributions are:
            
            <li>We propose a learning-based solution to generate synthetic events starting from video sequences.</li>
            <li>We show that the events generated with this method accurately reproduce the corresponding real events 
              in most of the cases, although being sensitive to noise.</li>
            <li>We evaluate our method on an object classification task and show that models trained on the synthetic 
              events generated with the proposed network perform better than 
              <a href="https://tub-rip.github.io/eventvision2021/papers/2021CVPRW_V2E_From_Video_Frames_to_Realistic_DVS_Events.pdf">Vid2E</a> 
              in terms of accuracy.</li>
              
            <div align="center"><img src="assets/img/semesterproject_generalidea.png" width="80%" class="center"/></div>

          </p>
        </div>
      </div>
    </div>
    <hr>
    
      <!-- Method -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <br>
          <h2 align="center" class="title is-3"><b>Method</b></h2>
          <br>
          <!-- <div align="center"><img src="assets/img/pvcnn_rq.png" width="90%" class="center"/></div> -->
          <div class="content has-text-justified">
            <p>
              In this section we present Spike-ESIM, a novel Spiking Neural Network for event 
              generation from video frames. The proposed network is different from traditional 
              Spiking Neural Networks in the way that it does not take as input trains of 
              asynchronous spikes, but rather sequences of upsampled video frames which are fed 
              directly at the level of the membrane potential of the first layer. In this way, 
              we skip the Post Synaptic Potential layer at the initial stage of the network, 
              which normally convolves the input spikes of each node with the temporal ε(·) 
              kernel to generate the potentials.
              
              <div align="center"><img src="assets/img/semesterproject_dynamicsneuron.png" width="60%" class="center"/></div>
              
              The network is a branched architecture, with each branch modeling a specific 
              polarity of the output event tensor. Modeling the event generation process with a 
              single network would not be feasible, since we need to account for both positive 
              and negative changes of brightness along the frames.
              
              The way the input video frames are fed into the network is the following: starting 
              from a sequence of T consecutive frames, we compute both the positive and negative 
              difference of the intensities along adjacent frames, namely ∆I(t) and −∆I(t), and 
              we feed them into the two separate branches. Each branch is composed of a 3D convolution 
              layer followed by a spike function module, this latter being the one that actually 
              generates events by thresholding the scaled differences of brightness according to 
              the membrane potential hyper-parameter.

            <div align="center"><img src="assets/img/semesterproject_spikeESIMarch.png" width="60%" class="center"/></div>

            <br>
            </p>
          </div>
        </div>
      </div>
      <hr>


      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <br>
          <h2 align="center" class="title is-3"><b>Results</b></h2>
          <br>
          <!-- <div align="center"><img src="assets/img/pvcnn_rq.png" width="90%" class="center"/></div> -->
          <div class="content has-text-justified">

            In this chapter we provide experimental results to validate the effectiveness of the 
            proposed method. A qualitative evaluation is performed by visually comparing the 
            generated events to the ground truth events.
            
            Visualizing the generated events reveals a strong dependency of the method on background 
            noise present in most of the scenes of the HS-ERGB dataset. The amount of noise that is 
            modeled in the scene is strongly dependent on the membrane threshold that we set before 
            training, and its value is extremely important as it strikes a balance between how much 
            noise we allow in the results and how well we want to model the real events
            
            <div align="center"><img src="assets/img/semesterproject_result1.png" width="60%" class="center"/></div>
            <div align="center"><img src="assets/img/semesterproject_result2.png" width="60%" class="center"/></div>

          </div>
          <br>
        </div>
      </div>
      


</div>
</section>




</body>
</html>