---
layout: default
---

<!-- <!DOCTYPE html> -->
<html>

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <style>
      .justified-text {
          text-align: justify;
      }
  </style>
  <title>Justified Text Example</title>
</head>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
        <div class="column has-text-centered">
          <h1 align="center" class="title is-1 publication-title"><b>Monocular Markerless 6D Pose Estimation <br>of ANYmal</b></h1>
          <div class="column is-full_width">
          </div>
          <br>
          <div align="center" class="is-size-5 publication-authors">
                <h5 align="center" class="title is-1 publication-title">
                  <b>
                <a href="https://tenhearts.github.io">Kexin Shi</a></span></span>&nbsp;&nbsp;&nbsp;&nbsp;
              <span class="author-block">
              <a href="https://ivanalberico.github.io/">Ivan Alberico</a></span>&nbsp;&nbsp;&nbsp;&nbsp;
              </span>
            <span class="author-block">
              <a href="https://mavt.ethz.ch/people/person-detail.MjY0MDU2.TGlzdC81NTksLTE3MDY5NzgwMTc=.html">Jonas Frey</a></span></span>&nbsp;&nbsp;&nbsp;&nbsp;
            <span class="author-block">
              <a href="https://mavt.ethz.ch/people/person-detail.MjE4MDk1.TGlzdC81NTksLTE3MDY5NzgwMTc=.html">Victor Klemm</a></span>&nbsp;&nbsp;&nbsp;&nbsp;
            </span>
            <span class="author-block">
              <a href="https://ethz.ch/en/the-eth-zurich/organisation/who-is-who/mavt/details.MTIxOTEx.TGlzdC8xOTEyLDEwNjE0ODE1NjU=.html">Marco Hutter</a>
            </span>
            </b>
            <br><br>
            <span class="author-block"><a href="https://rsl.ethz.ch/">Robotic Systems Lab, ETHz</a></span>
            </h5>
          </div>
          </div>
        </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <br><hr>
      <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <br>
        <h2 align="center" class="title is-3"><b>Abstract</b></h2>
        <div class="content has-text-justified">
          <p class="justified-text">
            <b>TL;DR: We build an accurate markerless toolbox to localize and estimate pose of <a href="https://www.anybotics.com/anymal-autonomous-legged-robot/">ANYmal</a>.</b>
          </p>
          <p class="justified-text">
            Localization is an important task when it comes to tracking robots accurately in complicated and changeable environments. 
            Previous methods generally rely on additional sources like depth cameras or QR codes placed in the surrounding environment. 
            In this work, we propose to remove the dependency from these external sources by deploying state-of-the-art 6D pose estimation deep 
            learning methods to achieve the same goal.
          </p>
          <br>
        </div>
      </div>
    </div>
  </div>
</section>
    

<section class="section">
  <div class="container is-max-desktop">
    <!-- Paper video. -->
    <div class="columns is-centered has-text-centered">
      <!-- <div class="column is-four-fifths"> -->
        <div class="column is-full_width">
          <hr><br>
        <h2 align="center" class="title is-3"><b>Video</b></h2>
        <br>
        <div align="center" class="video-container">
          <iframe width="560" height="315" src="https://www.youtube.com/embed/4saCcrD37qA?autoplay=1&controls=0" title="YouTube video player" 
          frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
          <!-- </video> -->
        </div>
      </div>
    </div>
    <!--/ Paper video. -->
    <br>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Key Idea -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <hr>
        <br>
        <h2 align="center" class="title is-3"><b>Key Idea</b></h2>
        <div class="content has-text-justified">
          <br>  
          <p class="justified-text">
            The way to achieve the final result comprises the following two building blocks:
            <li>Generate an accurate dataset including RGB images and ground truth 6D pose of the base of ANYmal;</li>
            <li>Train a 6D pose estimation network on the generated dataset with supervised learning.</li>
            <br>
            For detailed information, please refer to <a href="assets/pdf/plr_poster.pdf">poster</a> and <a href="assets/pdf/plr_report.pdf">report</a>.
          </p>
        </div>

        <br>
        <div align="center"><img src="assets/img/plr_key.png" width="60%" class="center"/></div>
      </div>
    </div>
    <hr>
    
      <!-- Method -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <br>
          <h2 align="center" class="title is-3"><b>Dataset Generation</b></h2>
          <br>
          <div class="content has-text-justified">
            <p class="justified-text">
              The ground truth poses of the base of ANYmal are generated by detecting AprilTags attached to a mounting fixed on top of the robot by leveraging the 
              ROS package <a href="http://wiki.ros.org/apriltag_ros">apriltag_ros</a>. Then the position and quaternion is averaged between multiple faces detected 
              in the same frame to reduce jitter.
              <br>
              <table><tr>
                <td><img src=assets/img/plr1.gif border=0 width="100%" ></td>
                <td><img src=assets/img/plr2.gif border=0 width="100%" ></td>
                </tr></table>   
            <br>
            </p>
          </div>
        </div>
      </div>
      <hr>

      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <br>
          <h2 align="center" class="title is-3"><b>Pose Estimation</b></h2>
          <br>
          
          <div class="content has-text-justified">
            <!-- <br>   -->
            <p class="justified-text">
              We adapt one of the state-of-the-art method for 6D pose estimation called <a href="https://arxiv.org/abs/2011.04307">EfficientPose</a> on the generated dataset.
              It is based on the one-stage object detection algorithm called <a href="https://github.com/xuannianz/EfficientDet">EfficientDet</a>, 
              and extends functionality to 6D pose estimation in a simple and intuitive way.
            </p>

            <p class="justified-text">  </p>

            <p class="justified-text"> 
              The network first extracts image features with the scalable backbone architecture 
              <a href="https://arxiv.org/abs/1905.11946">EfficientNet</a>. After that, the features 
              are fed into a bidirectional feature pyramid network (BiFPN) to extract and fuse features 
              at multiple scales. Finally, all features are fed into 4 sub-networks. Each block consists 
              of a classification network, a 2D bounding box regression network and additionally a 
              rotation and translation regression network.
            </p>

            <p class="justified-text">  </p>

            <p class="justified-text">
              What is required in order to train the network are the RGB images of ANYmal, the corresponding
              binary mask of the object of each frame, the 3D object model and the ground truth poses.
            </p>

            <p class="justified-text">  </p>
            
            <div align="center"><img src="assets/img/plr_network.png" width="60%" class="center"/></div>
          
          </div>
          <br>
        </div>
      </div>
      <hr>

      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <br>
          <h2 align="center" class="title is-3"><b>Results</b></h2>
          <br>
          <div class="content has-text-justified">

            <p class="justified-text"> 
              All experiments were conducted on selected sequences of the dataset, which were manually inspected 
              in order to filter out all the corner cases that might have affected the training procedure in a 
              negative way. We mainly focused our experimental analysis on the training of 3 sequences, of 
              approximately 2-3 minutes each, recorded at 30FPS in 4K. Before feeding the RGB sequences to the 
              network, each frame is first down-sampled to a resolution 640 X 480 in order to comply with the 
              <a href="https://arxiv.org/pdf/1509.01277.pdf">Linemod dataset</a> which represented the benchmark 
              for EfficientPose.
            </p>

            <p class="justified-text">  </p>

            <p class="justified-text">
              We evaluate our approach with the commonly used ADD metric, which computes the average point distances 
              between the 3D model point set M transformed with the ground truth and the predicted poses. In addition 
              to the ADD metric, we also evaluate the model with the ADD accuracy score, which represents the rate of 
              the ADD being smaller than 10% of the object diameter within each sequence.
            </p>

            <p class="justified-text">
              In order to guarantee that the presence of markers in the scene would not bring meaningful information to 
              the network while predicting the pose, we blur the cubes in the validation set and test the model on the 
              blurred frames. Visual inspection of the results by means of 3D bounding boxes centered and oriented according 
              to the predicted poses in each frame shows that the network generalizes well to unseen frames within the same 
              sequence
            </p>

            <p class="justified-text">  </p>
            <table><tr>
              <td><img src=assets/img/plr3.gif border=0 width="100%" ></td>
              <td><img src=assets/img/plr4.gif border=0 width="100%" ></td>
              </tr></table> 
            <table><tr>
              <td><img src=assets/img/plr5.gif border=0 width="100%" ></td>
              <td><img src=assets/img/plr6.gif border=0 width="100%" ></td>
              </tr></table> 
          </div>
          <br>
        </div>
      </div>
      


</div>
</section>




</body>
</html>