<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> | Ivan Alberico's personal website</title> <meta name="author" content="Ivan Alberico"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/autumn.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/icon.png"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://ivanalberico2.github.io//3dvision.html"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/">Ivan Alberico's personal website</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About</a> </li> <li class="nav-item"> <a class="nav-link " href="/assets/pdf/Ivan_CV.pdf"><span>CV</span></a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <meta charset="UTF-8"> <meta name="viewport" content="width=device-width, initial-scale=1.0"> <style>.justified-text{text-align:justify}</style> <title>Justified Text Example</title> <section class="hero"> <div class="hero-body"> <div class="container is-max-desktop"> <div class="column has-text-centered"> <h1 align="center" class="title is-1 publication-title"><b>End-2-End Self-Supervised SLAM</b></h1> <div class="column is-full_width"> </div> <br> <div align="center" class="is-size-5 publication-authors"> <h5 align="center" class="title is-1 publication-title"> <b> <a href="https://ivanalberico.github.io/" rel="external nofollow noopener" target="_blank">Ivan Alberico</a>     <span class="author-block"> <a href="https://www.linkedin.com/in/akbar96/" rel="external nofollow noopener" target="_blank">Mian Akbar Shah</a></span>     <span class="author-block"> <a href="https://www.linkedin.com/in/selim-kaelin-947887172/" rel="external nofollow noopener" target="_blank">Selim Kaelin</a></span>     <span class="author-block"> <a href="https://www.linkedin.com/in/emilk-sempertegui-9b7719100/" rel="external nofollow noopener" target="_blank">Emilk Sempertegui</a></span>     </b> <br><br> <span class="author-block"><a href="https://cvg.ethz.ch/" rel="external nofollow noopener" target="_blank">Computer Vision and Geometry LAB, ETH Zurich</a></span> </h5> </div> </div> </div> </div> </section> </div> <section class="section"> <div class="container is-max-desktop"> <br><hr> <div class="columns is-centered has-text-centered"> <div class="column is-four-fifths"> <br> <h2 align="center" class="title is-3"><b>Abstract</b></h2> <div class="content has-text-justified"> <p class="justified-text"> </p> <p class="justified-text"> Dense monocular SLAM is a challenging task since 3D structures reconstructed from monocular images are often sparse and It is not easy to recover reliable 3D information for non-textured regions in real-time. The use of a depth prediction network within a SLAM pipeline has been proposed to improve dense reconstructions (<a href="https://www.computer.org/csdl/proceedings-article/cvpr/2017/0457g565/12OmNzTH0Qa" rel="external nofollow noopener" target="_blank">CNN-SLAM</a>), however, deep learning models suffer from drops in accuracy on scenes not similar to the training ones (domain shifts). We present End-2-End Self-Supervised SLAM, a pipeline combining <a href="https://gradslam.github.io/" rel="external nofollow noopener" target="_blank">gradSLAM</a>, which enables SLAM systems to be posed as differentiable computational graphs, and online adaption to boost performance on indoor scenes. We train and test on two different datasets, implement an online adaptation module for refinement, explore the usage of uncertainty predictions and unsupervised scale learning, and supplement our work with experiments on weak supervision. Our framework generalizes well to previously unknown scenes, and through the online adaptation module we successfully address challenges related to indoor self-supervised depth estimation. </p> <br> </div> </div> </div> </div> </section> <section class="section"> <div class="container is-max-desktop"> <div class="columns is-centered has-text-centered"> <div class="column is-four-fifths"> <br> <h2 align="center" class="title is-3"><b>Introduction</b></h2> <br> <div class="content has-text-justified"> <p class="justified-text"> </p> <p class="justified-text"> A drawback of supervised deep networks, is that they need large labeled datasets to be properly trained. Creating labeled datasets for each of the countless SLAM applications is a laborious and expensive effort, which is why research has recently focused on some form of self-supervision The main challenges of self-supervised depth predition, especially in the context of indoor scenes, with which we are mainly concerned, are the following: </p> <li>Abundance of non-Lambertian surfaces and low-texture scenes that do not provide meaningful gradients.</li> <li>High degree of rotational movement in camera motion, which acts as noise.</li> <li>Scale inconsistency of pose estimates over different samples</li> <br> <p class="justified-text"> </p> <p class="justified-text"> In this work, we outline the following contributions: </p> <li>A self-supervised SLAM system based on PointFusion with gradient based learning.</li> <li>Implementation of an online adaptation module to refine our self-supervised depth predictions.</li> <li>Investigation into uncertainty predictions, scale learning and output fine-tuning to improve depth estimation and reconstruction.</li> </div> </div> </div> <hr> <div class="columns is-centered has-text-centered"> <div class="column is-four-fifths"> <br> <h2 align="center" class="title is-3"><b>Method</b></h2> <br> <div class="content has-text-justified"> <p class="justified-text"> In this section we provide a detailed insight into our online adaption module which is basically self-supervised depth estimation through novel view synthesis. We first explain the basic building blocks for self-supervised depth estimation and then present self-supervision from the global point cloud created by the <a href="https://arxiv.org/pdf/1711.10871.pdf" rel="external nofollow noopener" target="_blank">PointFusion</a> method. </p> <p class="justified-text"> </p> <div align="center"><img src="assets/img/3dvision_FullModel.PNG" width="100%" class="center"></div> <p class="justified-text"> </p> <p class="justified-text"> Due to the differentiable nature of gradSLAM, we can leverage the global reconstructed pointcloud to refine our depth maps in an online fashion. This task can be easily coupled with the novel view synthesis framework from the previous section. However, since we will compare pointclouds, this supervision incurs additional cost at each refinement step. The global pointcloud $G$ contains points that are updated after online adaption has been applied to the previous key-frames. Subsequently, these points are also refined through depth map fusion within the PointFusion based SLAM. Therefore, these points can be used to supervise future key-frames. </p> </div> </div> </div> <hr> <div class="columns is-centered has-text-centered"> <div class="column is-four-fifths"> <br> <h2 align="center" class="title is-3"><b>Results</b></h2> <br> <div class="content has-text-justified"> <p class="justified-text"> In this section we provide experimental results to validate the effectiveness of online refinement module along with the corresponding 3D reconstruction from PointFusion. </p> <p class="justified-text"> </p> <table><tr> <td> Depth predictions and reconstructed point clouds of a pair of key-frames taken from the ICL dataset. <p class="justified-text"> </p> <b>Left</b>: Ground truth <b>Center</b>: No online-refinement <b>Right</b>: Online-refinement <div align="center"><img src="assets/img/3dvision_ICL_Results.PNG" width="70%" class="center"></div> </td> </tr></table> <p class="justified-text"> </p> <p class="justified-text"> </p> <table><tr> <td> Depth predictions and reconstructed point clouds of a pair of key-frames taken from the TUM dataset. <p class="justified-text"> </p> <b>Left</b>: Ground truth <b>Center</b>: No online-refinement <b>Right</b>: Online-refinement <div align="center"><img src="assets/img/3dvision_TUM_Results.PNG" width="70%" class="center"></div> </td> </tr></table> <p class="justified-text"> </p> <p class="justified-text"> </p> <p class="justified-text"> </p> <p class="justified-text"> An addition to our method consists of integrating weak-supervision into the pipeline through ground truth depth information. We wanted to investigate the effectiveness of having pseudo-labels from old classic techniques, that could act as a supervisory signal for the depth predictions. However, due to time constraints we relied on sparse ground truth depth labels. In our case we used only 1% of the total amount of pixels. The weak supervision is integrated in our model through an L1 loss computed between the sparse ground truth values and the depth predictions from our network. </p> <p class="justified-text"> </p> <p class="justified-text"> </p> <table><tr> <td> The sequence of images shows a comparison between the ground truth depth map and the depth prediction after including weak-supervision into the pipeline (1% of the pixels). The selected keyframe is taken from the ICL-NUIM dataset. (a) Ground truth depth map (b) Sparse ground truth values (c) Predicted depth map. <p class="justified-text"> </p> <div align="center"><img src="assets/img/3dvision_weak-supervision.png" width="70%" class="center"></div> </td> </tr></table> <p class="justified-text"> </p> <p class="justified-text"> </p> <table><tr> <td> Comparison between global pointcloud reconstructions without (left) and with (right) weak supervision of a sequence of frames taken from the ICL-NUIM dataset. <p class="justified-text"> </p> <div align="center"><img src="assets/img/3dvision_comparison-demo-ICL.png" width="70%" class="center"></div> </td> </tr></table> <br> </div> </div> </div> <hr> </div> </section> </body> </html><html> <footer class="sticky-bottom mt-5"> <div class="container"> © Copyright 2024 Ivan Alberico. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme.Last updated: January 26, 2024. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </html>